# üß† FirstEye ‚Äî Model Development
**ARM AI Developer Challenge: Edge Model Optimization Track**

Welcome to the Model Development component of FirstEye, an AI system designed for ultra-efficient person presence detection across three Arm-powered platforms:
- üì± Android (Cortex-A720)
- üçì Raspberry Pi 5 (Cortex-A76)
- üéØ Arduino Nicla Vision (Cortex-M7 Microcontroller)

This directory contains everything related to the design, training, optimization, and evaluation of the FirstEye model.

---

## üöÄ Overview

The ARM hackathon focuses on building performant, deployable AI models for Arm-based devices, emphasizing:
- Small memory footprint
- Low latency
- Energy-efficient inference
- Portability across heterogeneous Arm hardware

To achieve this, we designed a compact CNN model with:
‚≠ê Input resolution: 80 √ó 80 RGB
‚≠ê Output: 2 classes ‚Üí `Person`, `No Person`
‚≠ê Format: Fully quantized TFLite (uint8)
‚≠ê Size: ~90 KB
‚≠ê Hardware-friendly ops: Conv2D, DepthwiseConv, ReLU, AvgPool

This model has been successfully deployed on:
- Android phone (real-time inference using CameraX + TFLite)
- Arduino Nicla Vision (OpenMV ML Engine + TensorFlow Lite Micro)
- Raspberry Pi 5 (TFLite Runtime + libcamera streaming)

This demonstrates the model‚Äôs true cross-platform edge deployment capability, which is a core judging criterion of this hackathon.

## üß© Model Architecture Summary
The architecture is based on principles of:
- Depthwise separable convolutions
- Early feature extraction
- Aggressive spatial downsampling
- Minimal parameter count
- Quantization-aware training

The model training script:
```
model_centric_track_small.py
```

implements the full training pipeline:
- Data preprocessing
- Model definition
- Training loop
- Evaluation
- Export to SavedModel and TFLite
- Post-training quantization

Final exported model:
```
wv_k_8_c_5_80_small.tflite
```

## üéØ Why This Model is Ideal for ARM Edge Devices

| Platform                 | Reason It Works                                                     |
| ------------------------ | ------------------------------------------------------------------- |
| **Android (A720)**       | Exploits TFLite NNAPI acceleration, <1ms overhead per frame.        |
| **Raspberry Pi 5 (A76)** | Optimized for CPU-only inference at 50‚Äì80 FPS using TFLite Runtime. |
| **Nicla Vision (M7)**    | Fits inside RAM, uses CMSIS-NN via OpenMV, ~15 FPS sustained.       |

This cross-platform operability is intentional and aligns with hackathon categories around:
- Model portability
- Efficient inference on constrained hardware
- Real-world deployability

## üì¶ Structure of This Directory
```
model/
‚îÇ   README.md                 ‚Üê this file
‚îÇ   Metrics.xlsx              ‚Üê accuracy & inference benchmarks
‚îÇ   TechnicalReport.md        ‚Üê full architecture description
‚îÇ   model_centric_track_small.py
‚îÇ   wv_k_8_c_5_80_small.tflite ‚Üê final deployed model
‚îÇ
‚îú‚îÄ‚îÄ wv_k_8_c_5/
‚îÇ   ‚îú‚îÄ‚îÄ *.tflite
‚îÇ   ‚îú‚îÄ‚îÄ *.pb
‚îÇ   ‚îî‚îÄ‚îÄ variables/
‚îÇ
‚îî‚îÄ‚îÄ wv_k_8_c_5_80_small.tf/
    ‚îú‚îÄ‚îÄ saved_model.pb
    ‚îú‚îÄ‚îÄ fingerprint.pb
    ‚îî‚îÄ‚îÄ variables/
```

## üõ†Ô∏è Training the Model

Training is optional for users of FirstEye; the repo already includes the final `.tflite` model.

But if you want to reproduce, retrain, or modify the model:

#### 1Ô∏è‚É£ Install Docker

Follow: https://docs.docker.com/engine/install/

#### 2Ô∏è‚É£ CPU Training (simplest)
```
sudo docker run -it --rm \
  -v $PWD:/workspace -w /workspace \
  tensorflow/tensorflow:latest \
  python model_centric_track_small.py
```

#### 3Ô∏è‚É£ GPU Training (if CUDA is available)
```
sudo docker run --gpus all -it --rm \
  -v $PWD:/workspace -w /workspace \
  tensorflow/tensorflow:latest-gpu \
  python model_centric_track_small.py
```

Outputs will appear in:
```
wv_k_8_c_5_80_small.tf/
wv_k_8_c_5_80_small.tflite
```

## üìä Evaluation & Metrics

The file Metrics.xlsx includes:
- Accuracy on held-out validation set
- Confusion matrix
- TFLite inference time on:
  - Nicla Vision (Cortex-M7)
  - Raspberry Pi 5 (Cortex-A76)
  - Android (A720 phone)

Our model demonstrates strong performance while staying extremely small.

## üîó Integration With Deployment Targets

The model from this folder is used directly in:

| Platform           | Path                                         | Link                                                                           |
| ------------------ | -------------------------------------------- | ------------------------------------------------------------------------------ |
| **Android**        | `deployment/Mobile-ARM-Cortex-A720/app`      | ‚Üí [Android Deployment Guide](../deployment/Mobile-ARM-Cortex-A720/README.md)   |
| **Nicla Vision**   | `deployment/Arduino_Nicla_Vision-Cortex-M7/` | ‚Üí [Nicla Vision Guide](../deployment/Arduino_Nicla_Vision-Cortex-M7/README.md) |
| **Raspberry Pi 5** | `deployment/RaspberryPi-Cortex-A76/`         | ‚Üí [Raspberry Pi 5 Guide](../deployment/RaspberryPi-Cortex-A76/README.md)       |


The same model file runs unchanged on all three devices ‚Äî a core demonstration of Arm-based cross-platform ML deployment.

## ü§ù Contributions & Extensions
You may extend this model by:
- Adding MobileNetV3-style inverted residual blocks
- Pruning channels for even smaller memory footprint
- Trying int4 or mixed-precision quantization
- Adding post-processing (temporal smoothing, motion detection, etc.)
Pull requests (PRs) are welcome.

## üì¨ Support

If you need help with model training or deployment:
‚Üí Open an issue in the repository.
‚Üí Or contact via GitHub Discussions.

## üåü Final Note

This model is the heart of FirstEye ‚Äî a tiny, fast, cross-device person detector built specifically for Arm CPUs and microcontrollers.
It showcases:
- ML model design
- Hardware-aware optimization
- Real-world deployment engineering

All of which are key in the Arm AI Developer Challenge.